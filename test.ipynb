{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, BatchNormalization, Activation, Input, Concatenate\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import InputLayer, Reshape, Flatten, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate segmentation for image\n",
    "import skimage\n",
    "import skimage.io\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import utils\n",
    "from utils import *\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load the datasets\n",
    "    x_train = np.load('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/x_train_full.npy')\n",
    "    y_train = np.load('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/y_train_full.npy')\n",
    "\n",
    "    x_test = np.load('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/x_test_full.npy')\n",
    "    y_test = np.load('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/y_test_full.npy')\n",
    "\n",
    "    x_val = np.load('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/x_val_full.npy')\n",
    "    y_val = np.load('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/y_val_full.npy')\n",
    "\n",
    "    input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val, input_shape\n",
    "\n",
    "\n",
    "def setup_training_hyperparameters():\n",
    "    # Set training hyperparameters\n",
    "    epochs = 1\n",
    "    lambda_ = 1\n",
    "    lr = 0.001\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_fn = weighted_scce_loss \n",
    "    batch_size = 512\n",
    "\n",
    "    return epochs, lambda_, lr, opt, loss_fn, batch_size\n",
    "\n",
    "class DataPartitioner:\n",
    "    def __init__(self, x_val, y_val, num_partitions, mode):\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.num_partitions = num_partitions\n",
    "        self.val_samples = len(x_val)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.wt_file = []\n",
    "        self.mode = mode\n",
    "\n",
    "    def get_partition(self):\n",
    "        for i in range(0, self.num_partitions):\n",
    "            idx = int(1.0 / self.num_partitions * self.val_samples) #set number of samples for each partition\n",
    "            start = idx * i                                         #set the starting index for a partition\n",
    "            end = (idx) * (i+1)                                     #set the concluding index for a partition\n",
    "            X = self.x_val[start:end,:,:,:]                         #store partitioned data array into a variable\n",
    "            y = self.y_val[start:end]                               #store partitioned label array into a variable\n",
    "            self.data.append(X)                                     #populate the data list with the data arrays\n",
    "            self.labels.append(y)                                   #populate the label list with the label arrays\n",
    "        for j in range(0,self.num_partitions+1):    \n",
    "            wt = 'session_'+str(j)+'_'+str(self.mode)  \n",
    "            self.wt_file.append(wt)                                 #store session names in a list\n",
    "        return self.data, self.labels, self.wt_file\n",
    "    \n",
    "class IncrementalLearning:\n",
    "    def __init__(self, x_train, y_train, partitioner, model, num_sample, \n",
    "                 train_opts, save_dir, wt_file, data, labels, mode, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.partitioner = partitioner\n",
    "        self.model = model\n",
    "        # self.fisher_samples = fisher_samples\n",
    "        self.num_sample = num_sample\n",
    "        self.train_opts = train_opts\n",
    "        self.save_dir = save_dir\n",
    "        self.wt_file = wt_file\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def _print_wrong_predictions(self, val_data, val_labels, model):\n",
    "        i = 1\n",
    "        for d, l in zip(val_data, val_labels):\n",
    "            y_pred = np.argmax(model.predict(d), axis=1)\n",
    "            wrong_pred = np.where(y_pred != l)\n",
    "            print('Number of incorrect predictions in validation set ' + repr(i) + ': ' + repr(len(wrong_pred[0])))\n",
    "            i = i + 1\n",
    "        print('\\n')\n",
    "\n",
    "    def _print_test_performances(self):\n",
    "        for i in range(self.partitioner.num_partitions):\n",
    "            model = tf.keras.models.load_model(self.save_dir + self.wt_file[i],\n",
    "                                               custom_objects={\"weighted_scce_loss\": weighted_scce_loss})\n",
    "            y_pred = np.argmax(model.predict(self.x_test), axis=1)\n",
    "            y_true = self.y_test\n",
    "            test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "            print('Test set accuracy after session ' + repr(i + 1) + ': ' + repr(round(100 * test_acc, 2)))    \n",
    "\n",
    "    def _base_train(self):\n",
    "            # start baseline training with regular sample weights\n",
    "            sample_weight = np.ones((self.x_train.shape[0],))\n",
    "            train_base = tf.data.Dataset.from_tensor_slices((self.x_train, self.y_train, \n",
    "                                                            sample_weight)).shuffle(self.x_train.shape[0]).batch(\n",
    "                                                                                    self.train_opts.get('batch_size'))\n",
    "            test_base = (self.data[0], self.labels[0]) #test on the first validation partition\n",
    "            trn = Train(self.train_opts.get('optimizer'), self.train_opts.get('loss_fn'))\n",
    "            acc_base = trn.train(self.model, self.train_opts.get('epochs'), \n",
    "                                 train_base, test_tasks=[test_base], \n",
    "                                 model_save_fname=self.save_dir+'session_0_'+str(self.mode))\n",
    "            return self.model\n",
    "            \n",
    "    def _train_partitions(self, model, fisher_samples=None):\n",
    "        for i in range(self.partitioner.num_partitions):\n",
    "            model = tf.keras.models.load_model(self.save_dir + self.wt_file[i],\n",
    "                                               custom_objects={\"weighted_scce_loss\": weighted_scce_loss})  \n",
    "            new_train, new_labels, sample_weights = augment_incorrect_samples(self.x_train, self.y_train, \n",
    "                                                                              [self.data[i]], [self.labels[i]], \n",
    "                                                                              model, self.mode)\n",
    "            if i==0: val_data = [self.data[i]]; val_labels=[self.labels[i]]\n",
    "            else: \n",
    "                val_data = []; val_labels = []\n",
    "                val_data.extend(self.data[0:i+1]); val_labels.extend(self.labels[0:i+1])\n",
    "            \n",
    "            self._print_wrong_predictions(val_data, val_labels, model)\n",
    "            if self.mode == 'ewc':\n",
    "                ewc = EWC(model, fisher_samples, num_sample=self.num_sample)\n",
    "                f_matrix = ewc.get_fisher()\n",
    "            else: f_matrix = None\n",
    "\n",
    "            train = tf.data.Dataset.from_tensor_slices((new_train, new_labels, \n",
    "                                                        sample_weights)).shuffle(new_train.shape[0]).batch(\n",
    "                                                                                    self.train_opts.get('batch_size'))\n",
    "            prior_weights = model.get_weights()\n",
    "            print('\\n [INFO] Starting Training Session '+repr(i+1))   \n",
    "\n",
    "            trn = Train(self.train_opts.get('optimizer'), self.train_opts.get('loss_fn'), \n",
    "                        prior_weights=prior_weights, lambda_=self.train_opts.get('lambda_'))\n",
    "            acc = trn.train(model, self.train_opts.get('epochs'), train, \n",
    "                            fisher_matrix=f_matrix, test_tasks=[(self.x_test, self.y_test)],\n",
    "                            model_save_fname = self.save_dir+self.wt_file[i+1])\n",
    "            print('[INFO] TEST ACC: {}'.format(acc))   \n",
    "\n",
    "            if i != self.partitioner.num_partitions - 1:\n",
    "                fisher_samples = gen_fisher_samples(new_train, new_labels, model)    \n",
    "          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--sessions\", type=int, help='Provide the number of incremental sessions')\n",
    "    parser.add_argument(\"--mode\", type=str, help='Provide method to use for IL (options are trad, wl or ewc)')\n",
    "    parser.add_argument(\"--folder\", type=str, help='Provide the folder name to save the trained models')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    num_partitions = args.sessions\n",
    "    mode = args.mode\n",
    "    folder = args.folder\n",
    "\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val, input_shape = load_data()\n",
    "    # x_train = x_train[0:1000,:,:,:]\n",
    "    # y_train = y_train[0:1000]\n",
    "    # x_val = x_val[0:100,:,:,:]\n",
    "    # y_val = y_val[0:100]\n",
    "\n",
    "    epochs, lambda_, lr, opt, loss_fn, batch_size = setup_training_hyperparameters()\n",
    "\n",
    "    num_sample = int(0.05 * x_train.shape[0])\n",
    "\n",
    "    # Set the folder path to save the model files\n",
    "    output_dir = '/home/audio_ml.work/data/audio/speech/speech_commands_arrays/Trained_Models/EUSIPCO/ewc/Models/'+str(folder)+'/'\n",
    "\n",
    "    partitioner = DataPartitioner(x_val=x_val, y_val=y_val, num_partitions=num_partitions, mode=mode)\n",
    "    data, labels, wt_file = partitioner.get_partition()\n",
    "    conv = conv_model(input_shape=input_shape, num_classes=35)\n",
    "    model = conv.get_compiled_model(opt, loss_fn, ['sparse_categorical_accuracy'])\n",
    "\n",
    "    trainer = IncrementalLearning(x_train, y_train, partitioner, model, num_sample,\n",
    "                                  {'optimizer': opt, 'loss_fn': loss_fn, 'epochs': epochs, \n",
    "                                   'lambda_': lambda_, 'batch_size': batch_size},\n",
    "                                  output_dir, wt_file, data, labels, mode, x_test, y_test)\n",
    "    model = trainer._base_train()\n",
    "\n",
    "    # # generate the fisher samples from EWC\n",
    "    if mode == 'ewc':\n",
    "        fisher_samples = gen_fisher_samples(x_train, y_train, model)    #generate fisher samples after baseline training. It will be forwarded to IL training if the mode is 'ewc'\n",
    "\n",
    "    trainer._train_partitions(model, fisher_samples)\n",
    "\n",
    "    model = tf.keras.models.load_model(output_dir+wt_file[-1],          #load the final session weights \n",
    "                                       custom_objects = {\"weighted_scce_loss\": weighted_scce_loss})\n",
    "    print('\\n')\n",
    "    trainer._print_wrong_predictions(data, labels, model)\n",
    "    print('\\n')\n",
    "    trainer._print_test_performances()\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
