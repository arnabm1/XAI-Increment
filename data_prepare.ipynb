{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python371064bitaudiomlvenvvirtualenv283d58706df347af9b3489b61c79c71b",
   "display_name": "Python 3.7.10 64-bit ('audio_ml_venv': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_list(main_dir, filepath):\n",
    "    # opening the file in read mode\n",
    "    file = open(filepath, \"r\")\n",
    "  \n",
    "    # reading the file\n",
    "    data = file.read()\n",
    "    initial_list =  data.split(\"\\n\")\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for items in initial_list:\n",
    "        data_list.append(main_dir + items)\n",
    "        # data_list.append(items)\n",
    "    return tf.convert_to_tensor(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/home/audio_ml.work/data/audio/speech/speech_commands/'\n",
    "data_dir = pathlib.Path(main_dir)\n",
    "# train_path = '/ifxhome/mazumderarna/XAI/training_list.txt'\n",
    "# val_path = '/ifxhome/mazumderarna/XAI/validation_list.txt'\n",
    "# test_path = '/ifxhome/mazumderarna/XAI/testing_list.txt'\n",
    "train_path = '/home/audio_ml.work/data/audio/speech/speech_commands/training_list.txt'\n",
    "val_path = '/home/audio_ml.work/data/audio/speech/speech_commands/validation_list.txt'\n",
    "test_path = '/home/audio_ml.work/data/audio/speech/speech_commands/testing_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Commands: ['on' 'marvin' 'up' 'two' 'right' 'nine' 'four' 'yes' 'five' 'six' 'zero'\n 'dog' 'down' 'house' 'learn' 'seven' 'forward' 'eight' 'follow' 'visual'\n 'one' 'off' 'sheila' 'happy' 'stop' 'tree' 'left' 'bird' 'three'\n 'backward' 'no' 'wow' 'cat' 'go' 'bed']\n"
     ]
    }
   ],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "commands = commands[commands != 'LICENSE']\n",
    "commands = commands[commands != 'training_list.txt']\n",
    "commands = commands[commands != 'validation_list.txt']\n",
    "commands = commands[commands != 'testing_list.txt']\n",
    "commands = commands[commands != '_background_noise_']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = create_set_list(main_dir, train_path)\n",
    "val_filenames = create_set_list(main_dir, val_path)\n",
    "test_filenames = create_set_list(main_dir, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = train_filenames[:-1]\n",
    "val_filenames = val_filenames[:-1]\n",
    "test_filenames = test_filenames[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[b'/home/audio_ml.work/data/audio/speech/speech_commands/backward/0165e0e8_nohash_0.wav'\n b'/home/audio_ml.work/data/audio/speech/speech_commands/backward/017c4098_nohash_0.wav'\n b'/home/audio_ml.work/data/audio/speech/speech_commands/backward/017c4098_nohash_1.wav'\n ...\n b'/home/audio_ml.work/data/audio/speech/speech_commands/zero/ffd2ba2f_nohash_3.wav'\n b'/home/audio_ml.work/data/audio/speech/speech_commands/zero/ffd2ba2f_nohash_4.wav'\n b'/home/audio_ml.work/data/audio/speech/speech_commands/zero/fffcabd1_nohash_0.wav'], shape=(84842,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "    # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "    # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "    audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "    # Since all the data is single channel (mono), drop the `channels`\n",
    "    # axis from the array.\n",
    "    return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(\n",
    "        input=file_path,\n",
    "        sep=os.path.sep)\n",
    "    # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
    "    # to work in a TensorFlow graph.\n",
    "    return parts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform_and_label(file_path):\n",
    "    label = get_label(file_path)\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform = decode_audio(audio_binary)\n",
    "    return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    # Zero-padding for an audio waveform with less than 16,000 samples.\n",
    "    input_len = 16000\n",
    "    waveform = waveform[:input_len]\n",
    "    zero_padding = tf.zeros(\n",
    "        [16000] - tf.shape(waveform),\n",
    "        dtype=tf.float32)\n",
    "    # Cast the waveform tensors' dtype to float32.\n",
    "    waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "    # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
    "    # clips are of the same length.\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    # Convert the waveform to a spectrogram via a STFT.\n",
    "    spectrogram = tf.signal.stft(\n",
    "        equal_length, frame_length=255, frame_step=128)\n",
    "    # Obtain the magnitude of the STFT.\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_and_label_id(audio, label):\n",
    "    spectrogram = get_spectrogram(audio)\n",
    "    label_id = tf.math.argmax(label == commands)\n",
    "    return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    output_ds = files_ds.map(\n",
    "        map_func=get_waveform_and_label)\n",
    "    output_ds = output_ds.map(\n",
    "        map_func=get_spectrogram_and_label_id)\n",
    "    return output_ds\n",
    "\n",
    "train_ds = preprocess_dataset(train_filenames)\n",
    "val_ds = preprocess_dataset(val_filenames)\n",
    "test_ds = preprocess_dataset(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(x):\n",
    "    x_min = x.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = x.max(axis=(1, 2), keepdims=True)\n",
    "    x = (x - x_min)/(x_max-x_min)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_numpy_arrays(data):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for audio, label in data:\n",
    "        x.append(audio.numpy())\n",
    "        y.append(label.numpy())\n",
    "\n",
    "    x = np.array(x)\n",
    "    x = min_max_scaling(x)\n",
    "    null = tf.zeros([x.shape[0],x.shape[1],x.shape[2],x.shape[3]], tf.float32)\n",
    "    x = tf.concat([x,null,null], 3)\n",
    "    # null = np.zeros((x.shape[0],x.shape[1],x.shape[2],x.shape[3]))\n",
    "    # x = np.concatenate([x,null,null], axis=3).astype('float32')\n",
    "    print('Shape of the dataset array: '+repr(x.shape))\n",
    "    y = np.array(y)\n",
    "    print('Shape of the label set array: '+repr(y.shape))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the dataset array: TensorShape([84842, 124, 129, 3])\nShape of the label set array: (84842,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = gen_numpy_arrays(train_ds)\n",
    "np.save('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/x_train_full.npy', x_train)\n",
    "np.save('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/y_train_full.npy', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the dataset array: TensorShape([11005, 124, 129, 3])\nShape of the label set array: (11005,)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = gen_numpy_arrays(test_ds)\n",
    "np.save('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/x_test_full.npy', x_test)\n",
    "np.save('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/y_test_full.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the dataset array: TensorShape([9981, 124, 129, 3])\nShape of the label set array: (9981,)\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val = gen_numpy_arrays(val_ds)\n",
    "np.save('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/x_val_full.npy', x_val)\n",
    "np.save('/home/audio_ml.work/data/audio/speech/speech_commands_arrays/new_split/y_val_full.npy', y_val)"
   ]
  }
 ]
}